{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 57 - HOW TO GENERATE FEATURES FOR Machine Learning \n",
    "\n",
    "# https://github.com/bnsreenu/python_for_microscopists/blob/master/057-ML_06_02_what%20are%20features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from scipy import ndimage as nd\n",
    "from skimage.filters import sobel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./camera/2006251215008901.jpg') #RGB image -> convert to single color\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.resize(img, (256, 256), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features vectors with Pandas df\n",
    "# Convert into one single list or array \n",
    "# Unwrap my image into a 1D array\n",
    "\n",
    "img2 = img.reshape(-1)\n",
    "df = pd.DataFrame()\n",
    "df['Original Pixel Values'] = img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entropy filter (lack of order), measure of disorder\n",
    "\n",
    "entropy_img = entropy(img, disk(1))\n",
    "entropy1 = entropy_img.reshape(-1)\n",
    "df['Entropy'] = entropy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gaussian filter\n",
    "\n",
    "gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "gaussian1 = gaussian_img.reshape(-1)\n",
    "df['Gaussian'] = gaussian1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sobel filter \n",
    "\n",
    "sobel_img = sobel(img)\n",
    "sobel1 = sobel_img.reshape(-1)\n",
    "df['Sobel'] = sobel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check other tutorials for Gabor filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Pixel Values</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Gaussian</th>\n",
       "      <th>Sobel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Pixel Values   Entropy  Gaussian  Sobel\n",
       "0                    189  1.584963       149    0.0\n",
       "1                    195  2.000000       145    0.0\n",
       "2                    161  2.000000       138    0.0\n",
       "3                    153  2.000000       129    0.0\n",
       "4                    111  2.000000       121    0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Each: \n",
    "\n",
    "cv2.imshow('Entropy', entropy_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imshow('Gaussian', gaussian_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imshow('Sobel', sobel_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51 - IMAGE SEGMENTATION USING K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check openCV K-means Clustering documentation\n",
    "\n",
    "im = cv2.imread('/Users/mia/Desktop/code/camera/2006251815008927.jpg')\n",
    "im = cv2.resize(im, (200,200),3)\n",
    "\n",
    "im2 = im.reshape((-1,3))\n",
    "im2 = np.float32(im2)\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "#Clusters\n",
    "k = 8\n",
    "\n",
    "#times algorithm is repeated\n",
    "attempts = 20 \n",
    "\n",
    "ret,label,center=cv2.kmeans(im2,k,None,criteria,attempts,cv2.KMEANS_PP_CENTERS) #can use PP_centers or random_centers for how center is assigned \n",
    "\n",
    "# Now convert back into uint8 (unsigned integers), and make original image\n",
    "center = np.uint8(center)\n",
    "# Flatten labels\n",
    "res = center[label.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape them\n",
    "res2 = res.reshape((im.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('segmented.jpg', res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('res2',res2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 - IMAGE SEGMENTATION USING GAUSSIAN MIXTURE MODEL (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 approaches into different umbrellas:\n",
    "\n",
    "# Unsupervised Learning\n",
    "# Figure out where features are\n",
    "\n",
    "# Supervised Learning \n",
    "# Supply a labeled image where you paint the pixels and label them\n",
    "# Save model and use to train future models\n",
    "\n",
    "# GMM is another example of Clustering technique falling into image/data processing\n",
    "#Clusters overlap then GMM is more suitable because K-means has hard assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./camera/2006251815008927.jpg')\n",
    "img = cv2.resize(im, (200,200),3)\n",
    "img2 = img.reshape((-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define instance, model\n",
    "# Step 2: Fit it to data you have\n",
    "# Step 3: Predict using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke my GMM\n",
    "gmm_model = GMM(n_components=2, covariance_type='tied').fit(img2)\n",
    "#type is also spherical or diagonal not only tied, depends on type of data. Check documentation\n",
    "\n",
    "#Generate labels\n",
    "gmm_labels = gmm_model.predict(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_shape = img.shape\n",
    "segmented = gmm_labels.reshape(original_shape[0], original_shape[1])\n",
    "cv2.imwrite('segmented_im.jpg', segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 66 - IMAGE SEGMENTATION - TRADITIONAL ML\n",
    "\n",
    "# Code adapted from: https://github.com/bnsreenu/python_for_microscopists/blob/master/062-066-ML_06_04_TRAIN_ML_segmentation_All_filters_RForest.py\n",
    "# STEP 1: FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to train SUPPORT VECTOR MACHINE for image segmentation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "img = cv2.imread('/Users/mia/Desktop/code/camera/2006251300008904.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to dataframe needed for our ML algorithm\n",
    "#Pixel value tells a lot about what each region is\n",
    "\n",
    "#Feature #1 is our original pixel values to the data frame added\n",
    "img2 = img.reshape(-1)\n",
    "df = pd.DataFrame()\n",
    "df['Original Image'] = img2\n",
    "#print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add other features \n",
    "\n",
    "# First set: Gabor Features (like gaussian or any kind of edge\n",
    "# detection features). Create a gabor filter bank to generate reponses onto the original image. \n",
    "\n",
    "\n",
    "# Generate Gabor features: this combination gives 32 different filters\n",
    "num = 1\n",
    "kernels = []\n",
    "for theta in range(2):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1,3):\n",
    "        for lamda in np.arange(0, np.pi, np.pi / 4):\n",
    "            for gamma in (0.05,0.5):\n",
    "                \n",
    "                gabor_label = 'Gabor' + str(num)\n",
    "                ksize = 5 #or 9\n",
    "                kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
    "                kernels.append(kernel)\n",
    "                #Now fikter the image and add values to a new column\n",
    "                fimg = cv2.filter2D(img2, cv2.CV_8UC3, kernel)\n",
    "                filtered_img = fimg.reshape(-1)\n",
    "                df[gabor_label] = filtered_img #labels columns as Gabor1, Gabor2 \n",
    "                print(gabor_label, ': theta=', theta, ':sigma=', sigma, ': lamda=', lamda, ': gamma=', gamma)\n",
    "                num += 1 #Increment for gabor column label\n",
    "\n",
    "print(df.head())                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other feature extractors: \n",
    "\n",
    "# Canny Edge: Edge detection \n",
    "edges = cv2.Canny(img, 100, 200)\n",
    "edges1 = edges.reshape(-1)\n",
    "df['Canny Edge'] = edges1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "\n",
    "#ROBERTS EDGE\n",
    "edge_roberts = roberts(img)\n",
    "edge_roberts1 = edge_roberts.reshape(-1)\n",
    "df['Roberts'] = edge_roberts1\n",
    "\n",
    "#SOBEL\n",
    "edge_sobel = sobel(img)\n",
    "edge_sobel1 = edge_sobel.reshape(-1)\n",
    "df['Sobel'] = edge_sobel1\n",
    "\n",
    "#SCHARR\n",
    "edge_scharr = scharr(img)\n",
    "edge_scharr1 = edge_scharr.reshape(-1)\n",
    "df['Scharr'] = edge_scharr1\n",
    "\n",
    "#PREWITT\n",
    "edge_prewitt = prewitt(img)\n",
    "edge_prewitt1 = edge_prewitt.reshape(-1)\n",
    "df['Prewitt'] = edge_prewitt1\n",
    "\n",
    "#GAUSSIAN with sigma=3\n",
    "from scipy import ndimage as nd\n",
    "gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "gaussian_img1 = gaussian_img.reshape(-1)\n",
    "df['Gaussian s3'] = gaussian_img1\n",
    "\n",
    "#GAUSSIAN with sigma=7\n",
    "gaussian_img2 = nd.gaussian_filter(img, sigma=7)\n",
    "gaussian_img3 = gaussian_img2.reshape(-1)\n",
    "df['Gaussian s7'] = gaussian_img3\n",
    "\n",
    "#MEDIAN with sigma=3\n",
    "median_img = nd.median_filter(img, size=3)\n",
    "median_img1 = median_img.reshape(-1)\n",
    "df['Median s3'] = median_img1\n",
    "\n",
    "#VARIANCE with size=3\n",
    "# slow part, we may not need that\n",
    "# variance_img = nd.generic_filter(img, np.var, size=3)\n",
    "# variance_img1 = variance_img.reshape(-1)\n",
    "# df['Variance s3'] = variance_img1  #Add column to original dataframe\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column that corresponds to the feature image\n",
    "# We need to tell what ground truth is (training)\n",
    "labeled_img = cv2.imread('/Users/mia/Downloads/2105011200000000.png')\n",
    "labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_BGR2GRAY)\n",
    "labeled_img1 = labeled_img.reshape(-1)\n",
    "df['Labels'] = labeled_img1\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: TRAINING A MODEL (USING RANDOM FOREST CLASSIFIER RFC)\n",
    "\n",
    "# Define dependent variable Y \n",
    "Y = df['Labels'].values\n",
    "\n",
    "# Define independent variable X\n",
    "X = df.drop(labels = ['Labels'], axis=1) #keep everything except Labels column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and train\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state=20)\n",
    "\n",
    "#Import ML algorithm and train model\n",
    "\n",
    "# here we use random forest as our classifier:\n",
    "# RandomForestRegressor works too but predicts a number value of data \n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#model = RandomForestClassifier(n_estimators=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# USE SVM Method ##############\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(max_iter=100) #try 1000 too (accuracy increases)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = model.predict(X_train)\n",
    "#use predict.proba for probability\n",
    "prediction_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics for accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "#compare\n",
    "#Y_test is our True \n",
    "print('Accuracy =', metrics.accuracy_score(Y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: SAVING TRAINED MODEL\n",
    "#Use this model to segment other images\n",
    "\n",
    "import pickle\n",
    "filename = 'ourfirst_model'\n",
    "pickle.dump(model, open(filename, 'wb')) # wb= write in binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to train: import it \n",
    "load_model = pickle.load(open(filename, 'rb')) #rb = read binary mode\n",
    "result = load_model.predict(X) #Predict all pixels in X\n",
    "\n",
    "segmented = result.reshape((img.shape))\n",
    "\n",
    "#Visualise\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(segmented, cmap='jet')\n",
    "plt.imsave('segmented_img.jpg', segmented, cmap='jet')\n",
    "\n",
    "#If I'm happy with this segmentation as a working image, let's apply\n",
    "# that to all the images and move to production mode. \n",
    "\n",
    "#Place all images in folder train_images in the Segmented folder \n",
    "#To do so, create a copy of this notebook file up until here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: SEGMENTING MULTIPLE IMAGES USING A SAVED MODEL\n",
    "\n",
    "#Feature extraction \n",
    "# Code to train random forest for image segmentation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#########\n",
    "# STEP 5 - Adding code here \n",
    "\n",
    "#Function:\n",
    "def feature_extraction(img): #img that we'll supply later\n",
    "    df = pd.DataFrame() #Create empty dataframe\n",
    "    \n",
    "    #Add original pixel values to df as feature #1\n",
    "    img2 = img.reshape(-1)\n",
    "    df['Original Image'] = img2\n",
    "    #print(df.head)\n",
    "\n",
    "#Add other features \n",
    "# First set: Gabor Features\n",
    "# Generate Gabor features\n",
    "    num = 1\n",
    "    kernels = []\n",
    "    for theta in range(2):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1,3):\n",
    "            for lamda in np.arange(0, np.pi, np.pi / 4):\n",
    "                for gamma in (0.05,0.5):\n",
    "                \n",
    "                    gabor_label = 'Gabor' + str(num)\n",
    "                    ksize = 5 #or 9\n",
    "                    kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
    "                    kernels.append(kernel)\n",
    "                #Now filter the image and add values to a new column\n",
    "                    fimg = cv2.filter2D(img2, cv2.CV_8UC3, kernel)\n",
    "                    filtered_img = fimg.reshape(-1)\n",
    "                    df[gabor_label] = filtered_img #labels columns as Gabor1, Gabor2 \n",
    "                    print(gabor_label, ': theta=', theta, ':sigma=', sigma, ': lamda=', lamda, ': gamma=', gamma)\n",
    "                    num += 1 #Increment for gabor column label\n",
    "\n",
    "#print(df.head())  \n",
    "# Add other feature extractors: \n",
    "    # Canny Edge: Edge detection \n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    edges1 = edges.reshape(-1)\n",
    "    df['Canny Edge'] = edges1\n",
    "\n",
    "    from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "\n",
    "    #ROBERTS EDGE\n",
    "    edge_roberts = roberts(img)\n",
    "    edge_roberts1 = edge_roberts.reshape(-1)\n",
    "    df['Roberts'] = edge_roberts1\n",
    "\n",
    "    #SOBEL\n",
    "    edge_sobel = sobel(img)\n",
    "    edge_sobel1 = edge_sobel.reshape(-1)\n",
    "    df['Sobel'] = edge_sobel1\n",
    "\n",
    "    #SCHARR\n",
    "    edge_scharr = scharr(img)\n",
    "    edge_scharr1 = edge_scharr.reshape(-1)\n",
    "    df['Scharr'] = edge_scharr1\n",
    "\n",
    "    #PREWITT\n",
    "    edge_prewitt = prewitt(img)\n",
    "    edge_prewitt1 = edge_prewitt.reshape(-1)\n",
    "    df['Prewitt'] = edge_prewitt1\n",
    "\n",
    "    #GAUSSIAN with sigma=3\n",
    "    from scipy import ndimage as nd\n",
    "    gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "    gaussian_img1 = gaussian_img.reshape(-1)\n",
    "    df['Gaussian s3'] = gaussian_img1\n",
    "\n",
    "    #GAUSSIAN with sigma=7\n",
    "    gaussian_img2 = nd.gaussian_filter(img, sigma=7)\n",
    "    gaussian_img3 = gaussian_img2.reshape(-1)\n",
    "    df['Gaussian s7'] = gaussian_img3\n",
    "\n",
    "    #MEDIAN with sigma=3\n",
    "    median_img = nd.median_filter(img, size=3)\n",
    "    median_img1 = median_img.reshape(-1)\n",
    "    df['Median s3'] = median_img1\n",
    "\n",
    "#VARIANCE with size=3\n",
    "# slow part, we may not need that\n",
    "# variance_img = nd.generic_filter(img, np.var, size=3)\n",
    "# variance_img1 = variance_img.reshape(-1)\n",
    "# df['Variance s3'] = variance_img1  #Add column to original dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "########\n",
    "# Everything from previous steps is useless because we are using\n",
    "# unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Import the pickled file\n",
    "filename = './2006251300008904.jpg'\n",
    "load_model = pickle.load(open(filename, 'rb')) #reading binary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step through the folder containing images that need to be segmented\n",
    "\n",
    "#Go through each file - multiple files\n",
    "#Same as steps before except we use the for loop to step through files\n",
    "path = 'images/Train_images/*.jpg'\n",
    "for file in glob.glob(path):\n",
    "    img1 = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Do feature extraction and call the function which returns a df\n",
    "    X = feature_extraction(img)\n",
    "    result = load_model.predict(X) #the pickled model and predict\n",
    "    segmented = result.reshape((img.shape)) #reshape to my original img shape\n",
    "    name = file.split(\"e_\")\n",
    "    plt.imsave('images/Segmented/' + name[1], segmented, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image segmentation approach where we generated a bunch of features\n",
    "# from our input images and then trained a ML algo model (in this case RF)\n",
    "# saved the model and used it to segment a whole bunch of images\n",
    "# in a folder.\n",
    "\n",
    "#Leveraging the greatness of Gabor filter using texture to \n",
    "# perform segmentation\n",
    "\n",
    "# for limited amount of training images, traditional ML \n",
    "# beats DL. If you have more than 100 images for training, then \n",
    "# use DL and U-net approach.\n",
    "\n",
    "# Source: https://github.com/bnsreenu/python_for_microscopists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "\n",
    "#What are Gabor filters? \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(kernel)\n",
    "\n",
    "kernel_resized = cv2.resize(kernel, (400,400)) \n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Filtered Image', fimg)\n",
    "cv2.imshow('Kernel', kernel_resized)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#df.to_csv('Gabor.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 67 - Feature based image segmentation using traditional Machine Learning\n",
    "\n",
    "# Code adapted from: https://github.com/bnsreenu/python_for_microscopists/blob/master/067-ML_06_05_PREDICT_ML_segmentation_All_filters_RForest.py\n",
    "\n",
    "# Multi-training images \n",
    "# Good idea to label masks and images the same way \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying trained model to segment multiple files. \n",
    "\n",
    "filename = \"ourfirst_model\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "path = \"./images/Train_images/*.jpg\"\n",
    "for file in glob.glob(path):\n",
    "    print(file)     #just stop here to see all file names printed\n",
    "    img1= cv2.imread(file)\n",
    "    img1 = cv2.resize(img1, (128,128))\n",
    "    img = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Call the feature extraction function.\n",
    "    X = feature_extraction(img)\n",
    "    result = loaded_model.predict(X)\n",
    "    segmented = result.reshape((img.shape))\n",
    "    \n",
    "    name = file.split(\"e_\")\n",
    "    plt.imsave('images/Segmented/'+ name[1], segmented, cmap ='jet')\n",
    "\n",
    "#Above, we are splitting the file path into 2 -> creates a list with 2 entries\n",
    "#Then we are taking the second half of name to save segmented images with that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(img):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#All features generated must match the way features are generated for TRAINING.\n",
    "#Feature1 is our original image pixels\n",
    "    img2 = img.reshape(-1)\n",
    "    df['Original Image'] = img2\n",
    "\n",
    "#Generate Gabor features\n",
    "    num = 1\n",
    "    kernels = []\n",
    "    for theta in range(2):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1, 3):\n",
    "            for lamda in np.arange(0, np.pi, np.pi / 4):\n",
    "                for gamma in (0.05, 0.5):\n",
    "#               print(theta, sigma, , lamda, frequency)\n",
    "                \n",
    "                    gabor_label = 'Gabor' + str(num)\n",
    "#                    print(gabor_label)\n",
    "                    ksize=9\n",
    "                    kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)    \n",
    "                    kernels.append(kernel)\n",
    "                    #Now filter image and add values to new column\n",
    "                    fimg = cv2.filter2D(img2, cv2.CV_8UC3, kernel)\n",
    "                    filtered_img = fimg.reshape(-1)\n",
    "                    df[gabor_label] = filtered_img  #Modify this to add new column for each gabor\n",
    "                    num += 1\n",
    "                    \n",
    "########################################\n",
    "\n",
    "#Generate OTHER FEATURES and add them to the data frame\n",
    "#Feature 3 is canny edge\n",
    "    edges = cv2.Canny(img, 100,200)   #Image, min and max values\n",
    "    edges1 = edges.reshape(-1)\n",
    "    df['Canny Edge'] = edges1 #Add column to original dataframe\n",
    "\n",
    "    from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "\n",
    "#Feature 4 is Roberts edge\n",
    "    edge_roberts = roberts(img)\n",
    "    edge_roberts1 = edge_roberts.reshape(-1)\n",
    "    df['Roberts'] = edge_roberts1\n",
    "\n",
    "#Feature 5 is Sobel\n",
    "    edge_sobel = sobel(img)\n",
    "    edge_sobel1 = edge_sobel.reshape(-1)\n",
    "    df['Sobel'] = edge_sobel1\n",
    "\n",
    "#Feature 6 is Scharr\n",
    "    edge_scharr = scharr(img)\n",
    "    edge_scharr1 = edge_scharr.reshape(-1)\n",
    "    df['Scharr'] = edge_scharr1\n",
    "\n",
    "    #Feature 7 is Prewitt\n",
    "    edge_prewitt = prewitt(img)\n",
    "    edge_prewitt1 = edge_prewitt.reshape(-1)\n",
    "    df['Prewitt'] = edge_prewitt1\n",
    "\n",
    "    #Feature 8 is Gaussian with sigma=3\n",
    "    from scipy import ndimage as nd\n",
    "    gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "    gaussian_img1 = gaussian_img.reshape(-1)\n",
    "    df['Gaussian s3'] = gaussian_img1\n",
    "\n",
    "    #Feature 9 is Gaussian with sigma=7\n",
    "    gaussian_img2 = nd.gaussian_filter(img, sigma=7)\n",
    "    gaussian_img3 = gaussian_img2.reshape(-1)\n",
    "    df['Gaussian s7'] = gaussian_img3\n",
    "\n",
    "    #Feature 10 is Median with sigma=3\n",
    "    median_img = nd.median_filter(img, size=3)\n",
    "    median_img1 = median_img.reshape(-1)\n",
    "    df['Median s3'] = median_img1\n",
    "\n",
    "    #Feature 11 is Variance with size=3\n",
    "    variance_img = nd.generic_filter(img, np.var, size=3)\n",
    "    variance_img1 = variance_img.reshape(-1)\n",
    "    df['Variance s3'] = variance_img1  #Add column to original dataframe\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
